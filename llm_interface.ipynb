{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e757cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e244accb-8fc9-4b2f-8b58-785e596aed78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c59f08-c10c-42ba-8f41-d81a16ac02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUESSED = False\n",
    "CORRECT_ANS = 'mbappe'\n",
    "MODEL_NAME = \"llama3.1:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0b3184c-3238-4ef4-be4a-1a0ce90dd2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed models: ['llama3:8b', 'mario:latest', 'mistral:7b', 'llama3.1:latest', 'steamdj/llama3.1-cpu-only:latest', 'mistral:latest', 'mixtral:latest', 'llama3:latest', 'snowflake-arctic-embed2:latest', 'mistral-small:latest', 'gemma3:27b', 'deepseek-r1:70b', 'deepseek-r1:32b']\n"
     ]
    }
   ],
   "source": [
    "models = requests.get(\"http://localhost:11434/api/tags\").json()\n",
    "installed = [m[\"name\"] for m in models[\"models\"]]\n",
    "print(\"Installed models:\", installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ad6ecbc-a4a2-4df1-b79f-9caae977e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========WELCOME TO THIS (un)PLEASANT GUESSING GAME=========\n",
      "\n",
      "Ask some clarification questions if you do not know what article could the image be from, or guess directly!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: You've received an image, and you need to guess what Wikipedia article it comes from. Go ahead and ask me any clarification questions you'd like (e.g., \"Is the article related to a movie?\", \"Is it a person?\", etc.) or propose an answer.\n",
      "\n",
      "(Remember, the correct answer is: {CORRECT_ANS})\n",
      "\n",
      "What would you like to do first?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " mbappe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YIPPI TU AS devinE !\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# Initialize client (connects to the Ollama service running locally)\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "\n",
    "def chat():\n",
    "    global conversation\n",
    "    print(\"=========WELCOME TO THIS (un)PLEASANT GUESSING GAME=========\\n\")\n",
    "    # TODO: send image!\n",
    "    print(\"Ask some clarification questions if you do not know what article could the image be from, or guess directly!\\n\")\n",
    "\n",
    "    conversation = [\n",
    "         {\"role\": \"system\", \"content\": \"\"\"\n",
    "         You are a guessing game host. The user has received an image and has to guess what Wikipedia article it comes from. They can ask you clarification questions and propose answers until they guess correctly.\n",
    "         The CORRECT ANSWER (that will terminate the game) is: {CORRECT_ANS}\n",
    "         The context to use for answering clarification questions: {ARTICLE_TEXT}\n",
    "         \"\"\"}\n",
    "    ]\n",
    "    while not GUESSED:\n",
    "        user_input = input()\n",
    "        if user_input.lower()==CORRECT_ANS:\n",
    "            print(\"YIPPI TU AS devinE !\")\n",
    "            break\n",
    "\n",
    "        # Stream response so you see it as it generates\n",
    "        stream = client.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=conversation + [{\"role\": \"user\", \"content\": user_input}],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        reply = \"\"\n",
    "        print(\"Answer: \", end=\"\", flush=True)\n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            reply += content\n",
    "            sys.stdout.write(content)\n",
    "            sys.stdout.flush()\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Update conversation history\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
